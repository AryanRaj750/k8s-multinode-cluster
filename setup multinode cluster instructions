# k8s-multinode-cluster
***
This is for setup over cloud e.g AWS. In your system python script file e.g ec2.py or other which can retrieve information from your AWS account about instances.
And then second important file is ec2.ini file. Both these files should be in your /etc/ansible/ direcory. and the ec2.py file should be executable (run chmod +x ec2.py to make executable)/
There should be not version confliction(e.g python file & ansible version) other face many problems.
Run /etc/ansible/ec2.py --list for checking properly working or not. if it gives all details of instances then it is working properly.
you have to expose some. Visit in aws doc dynamic invenory page there would be keyword
If you have to setup k8s cluster over AWS then inside vars/main.yml file you have to update information of your AWS account like region, sg_id.
for launching instances on AWS you have to run "ansible-playbook instance_launch.yml" file. Be careful you run this command this will be fail due to authentication/permision 
denied. so first you have to add your pem key file in /root/.ssh/ directory and you have to run "ssh-add key-file-name" like ssh-add aws.pem
This repo contains three roles 1. ec2Instance_launch  2. k8s_master,  3. k8s_worker

1. ec2Instance_launch role have for launching instances on AWS cloud. By default this role will three launch by tagging first instance as master. Second instance as worker node and 
third instance as workernode also.  Inside vars folder/directory you will get a variable named as 'tags' which is of list types. inside this variable i have passed tags name of 
each instance. So that, it can be easy to identify worker node by tag name. According to your choice you can pass tags name inside that list. But remember one thing that how many 
tags name you will pass in tags variable that number of instances will be launched.

2. k8s_master role:
This role will help to comfigure instance tagged with master as kubernetes master node. But if you want a specific tag for your master node then you can tagged but inside this
role in tasks directory you have to update hosts name as your tagged name. so, that this role can understand that which instance you want to configure as master node
example:-   in my case hosts: tag_node_master but in your case if you changed then it would be tag_key_value as hosts: tag_env_prod e.g key=env, value=prod.
There are two files inside files directory of role e.g daemon.json, kubernetes,repo.  daemon.json file is used to changed the driver of runtime. By default driver is
cgroupfs but k8s support systemd driver for runtime. for confirmation you can run "docker info | grep csgroupfs". so, you have to change cgroupfs driver as systemd. for this 
i have used daemon.json file so you have to need change it manually. The second file is kubernetes.repo, this file is used for reconfigure yum repository. so that we can install 
kubeadm , kubelet, kubectl package. 
In tasks main.yml, master node is initialize/configure with cidr=10.244.0.0/16 because flannel pods allow network of 10.244.0.0/16. But if you want to change cidr then you can
change but you have to update configmap file of flannel pod maually. otherwise, coreDns would not work. it would be in running state but not gives service of kube-proxy.

3. k8s_worker role:
This role would help to configure instances as workernode for k8s master. In this role inside files directory there would be three file daemon.json, kubernetes.repo and k8s.conf.
daemon.json and kubernetes.repo file are same whatever are in k8s_master role. but k8s.conf file is different file. k8s.conf file is used for some changing in kernel setting for 
bridge network connection. k8s.conf will enables bridge for connection. workernode will be join automatically to master node. you don't need to join it.

